{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPN1zySfDmB3i5ODDuuApq+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# LIBRARIES #\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import warnings\n",
        "import io\n",
        "import sklearn\n",
        "import sklearn.metrics  as metrics\n",
        "from google.colab import files\n",
        "from sklearn.cluster import KMeans\n",
        "import sklearn.cluster as cluster\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "SxHX6ZvxXiMy"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TyRv__coEMj"
      },
      "outputs": [],
      "source": [
        "# READ CSV #\n",
        "warnings.filterwarnings('ignore')\n",
        "uploaded = files.upload()\n",
        "data = pd.read_csv(io.BytesIO(uploaded['student-por.csv'])) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FIRST APPROACH TO THE DATASET #\n",
        "\n",
        "# columns names #\n",
        "data_names = ['school', 'sex', 'age', 'address', 'family_size', 'parent_cohabition_status', 'mother_education', 'father_education','mother_job', 'father_job', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'school_support', 'family_support', 'paid', 'extra_curricular_activities', 'nursery', 'higher', 'internet_access', 'romantic', 'famrel', 'freetime', 'going_out', 'workday_alcohol', 'weekend_alcohol', 'health', 'absences','grade_period1', 'grade_period2', 'final_grade' ]\n",
        "data.columns = data_names\n",
        "\n",
        "# numerical vs categorical columns #\n",
        "numerical_columns = ['age', 'mother_education', 'father_education', 'traveltime', 'studytime', 'failures', 'famrel', 'freetime', 'going_out', 'workday_alcohol', 'weekend_alcohol', 'health', 'absences','grade_period1', 'grade_period2', 'final_grade']\n",
        "categorical_columns = [\"sex\",'address', 'family_size', 'parent_cohabition_status','mother_job', 'father_job', 'reason', 'guardian',\"school_support\", \"family_support\",'paid', 'extra_curricular_activities', 'nursery', 'higher', 'internet_access', 'romantic']\n"
      ],
      "metadata": {
        "id": "mzmiX6k9oxvJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLEAN THE DATABASE # \n",
        "\n",
        "# 1. Drop the columns that we are not going to use.\n",
        "data_clean = data.drop(['grade_period1', 'grade_period2'], axis=1)\n",
        "\n",
        "# 2. Drop na's.\n",
        "data_clean=data.replace(r'^\\s*$', np.nan, regex=True)\n",
        "data_clean=data_clean.dropna(inplace=True)\n",
        "\n",
        "# 3. Get the dummies so that it is easier to work. \n",
        "# This is, transform the categorical columns into numerical ones. \n",
        "data_clean = pd.get_dummies(data, columns=categorical_columns)\n",
        "data_clean.columns\n",
        "data_clean.describe()\n",
        "\n",
        "# 4. Transform the data. \n",
        "# we need to standarize the data to take into consideration variations in measurments, units and scales. \n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data_clean[numerical_columns])\n",
        "scaled_dataframe = pd.DataFrame(scaled_data, columns = numerical_columns)  \n",
        "print(scaled_dataframe.columns)\n",
        "scaled_dataframe.head()\n",
        "\n",
        "for column in numerical_columns:\n",
        "  data_clean.append(scaled_dataframe[column])\n",
        "  data_clean[column] = scaled_dataframe[column]\n",
        "\n",
        "scaled_dataframe"
      ],
      "metadata": {
        "id": "qRLrP1mNsxf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CHOOSING THE OPTIMAL K VALUE #\n",
        "# there is various methods to choose the optimun value of k. We'll start using the elbow plot method,\n",
        "# and then proceed to the silhouette method. \n",
        "# The Elbow Method is more of a decision rule, while the Silhouette is a metric used for validation while clustering. \n",
        "\n",
        "# 1. ELBOW PLOT\n",
        "# the elbow plot alows us to plot inercia (measure of how well the data was clustered by the KM alg) against the number of clusters. \n",
        "def optimise_k(data,max_k):\n",
        "  means=[]\n",
        "  inertias=[]\n",
        "  for k in range(1,max_k):\n",
        "    kmeans=KMeans(n_clusters=k)\n",
        "    kmeans.fit(data)\n",
        "    means.append(k)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "  fig=plt.subplots(figsize=(10,5))\n",
        "  plt.plot(means,inertias,\"o-\")\n",
        "  plt.xlabel(\"Number of clusters\")\n",
        "  plt.ylabel(\"Inertia\")\n",
        "  plt.grid(True)\n",
        "  plt.title(\"Elbow plot\", size=18)\n",
        "  plt.show()\n",
        "\n",
        "optimise_k(scaled_dataframe,10)\n",
        "# Optimun value of k: 4. \n",
        "\n",
        "# 2. THE SILHOUETTE METHOD\n",
        "# The silhouette value measures how similar a point is to its own cluster (cohesion) compared to other clusters (separation).\n",
        "print(\"\\n\")\n",
        "print(\"Silhouette method:\")\n",
        "for i in range(2,10):\n",
        "  labels=cluster.KMeans(n_clusters=i, init=\"k-means++\",random_state=200).fit(scaled_dataframe).labels_\n",
        "  print(\"Silhouette score for k (clusters) = \"+str(i)+\" is \" + str(metrics.silhouette_score(scaled_dataframe,labels,metric=\"euclidean\", sample_size=1000,random_state=200)))\n",
        "\n"
      ],
      "metadata": {
        "id": "W52S3ZzGUuUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLUSTERING #\n",
        "\n",
        "# 1. APPLY K-MEANS CLUSTERING\n",
        "kmeans = KMeans(n_clusters=4)\n",
        "label=kmeans.fit(scaled_dataframe)\n",
        "\n",
        "# 2. ADD THE COLUMN OF THE CLUSTER (optional)\n",
        "scaled_dataframe[\"kmeans_4\"]=kmeans.labels_\n",
        "#scaled_dataframe\n",
        "\n",
        "# 3. PLOT THE RESULT\n",
        "# i can't get it done, i'm still trying. \n"
      ],
      "metadata": {
        "id": "QQdffIdxnztf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}